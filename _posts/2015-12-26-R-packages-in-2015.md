---
layout: post
title: Our R package roundup
categories: articles
excerpt: "The year 2015 in an RPR: R Packages Review!"
tags: [R, CRAN, packages]
published: false
comments: true
author: safferli 
---

# A year in review

<span class = "dropcap">I</span>t's the time of the year again where one eats too much, and gets in a reflective mood! 2015 is nearly over, and us bloggers here at opiateforthemass.es thought it would be nice to argue endlessly which R package was the best/neatest/most fun/most useful/most whatever in this year! 

Since we are in a festive mood, we decided we would not fight it out but rather present our top five of new R packages, a purely subjective list of packages we approve of. 

![gif](http://cdn.gifbay.com/2012/11/ok_chuck_norris-12381.gif)
{: .center}

But do not despair, dear reader! We have also pulled hard data on R package popularity from CRAN, and will present this first. 


# Top Popular CRAN packages

Let's start with some factual data before we go into our personal favourties of 2015. We'll pull the titles of the new 2015 R packages from [cranberries](http://dirk.eddelbuettel.com/cranberries/), and parse the CRAN downloads per day using `cranlogs` package.  

Using downloads per day as a ranking metric could have the problem that earlier package releases have had more time to create a buzz and shift up the average downloads per day, skewing the data in favour of older releases. Or, it could have the complication that younger package releases are still on the early "hump" part of the downloads (let's assume they'll follow a log-normal (exponential decay) distribution, which most of these things do), thus skewing the data in favour of younger releases. I don't know, and this is an interesting question I think we'll tackle in a later blog post... 

For now, let's just assume that average downloads per day is a relatively stable metric to gauge package success with. We'll grab the packages released using `rvest`: 

{% highlight r %}
berries <- read_html("http://dirk.eddelbuettel.com/cranberries/2015/")
titles <- berries %>% html_nodes("b") %>% html_text
new <- titles[grepl("^New package", titles)] %>% 
  gsub("^New package (.*) with initial .*", "\\1", .) %>% unique
{% endhighlight %}

and then `lapply()` these titles into the CRAN and parse their respective average downloads per day: 

{% highlight r %}
logs <- pblapply(new, function(x) {
  down <- cran_downloads(x, from = "2015-01-01")$count 
  if(sum(down) > 0) {
    public <- down[which(down > 0)[1]:length(down)]
  } else {
    public <- 0
  }
  return(data.frame(package = x, sum = sum(down), avg = mean(public)))
})

logs <- do.call(rbind, logs) 
{% endhighlight %}

With some quick `dplyr` and `ggplot` magic, these are the top 20 new CRAN packages from 2015, by average number of daily downloads: 

<!-- too tired: change colours so that not two are next to each other -->

![top 20 new CRAN packages in 2015]({{ site.url }}/images/cran-top20-2015.png)

The full code is availble [on github](https://github.com/Quiri/quiri.github.io/blob/master/_R/scripts/topcran.R), of course. 

As we can see, the main bias does not come from our choice of ranking metric, but by the fact that some packages are more "under the hood" and are pulled by many packages as dependencies, thus inflating the download statistics. 

The top four packages (`rversions`, `xml2`, `git2r`, `praise`) are all technical packages. Although I have to say I did not know of [`praise`](https://github.com/gaborcsardi/praise) so far, and it looks like it's a very fun package, indeed: you can automatically add randomly generated praises to your output! Fun times ahead, I'd say. 

Excluding these, the clear winner of "frontline" packages are `readxl` and `readr`, both packages by Hadly Wickham dealing with importing data into R. Well-deserved, in our opinion. These are packages nearly everybody working with data will need on a daily basis. Although, one hopes that contact with Excel sheets is kept to a minimum to ensure one's sanity, and thus `readxl` is needed less often in daily life!

The next two are packages (`DiagrammeR` and `visNetwork`) relate to network diagrams, something that seems to be *en vogue* currently.  R is getting some much-needed features on these topics here it seems. 

`plotly` is the R package to the [recently open-sourced](https://plot.ly/javascript/open-source-announcement/) popular plot.ly javascript libraries for interactive charts. A well-deserved top ranking entry! We also see packages that build and improve the ever-popular `shiny` packages (`DT` and `shinydashboard`), `leaflet` dealing with interactive mapping issues, and packages on [stan](http://mc-stan.org/), the Baysian statistical interference language (`rstan`, `StanHeaders`). 

But now, this blog's authors' personal top five of new R packages for 2015: 


# readr

<!-- safferli -->

https://github.com/hadley/readr


# infuser 

<!-- yuki -->

https://github.com/bart6114/infuser/


# googlesheets 

([Kirill's](/authors/#kirill) pick)

[googlesheets](https://github.com/jennybc/googlesheets) by [Jennifer Bryan](http://www.stat.ubc.ca/~jenny/) finally allows me to directly output to Google Sheets, instead of output it to xlsx format and then push it (mostly manually) to Google Drive. At our company we use Google Drive as a data communication and storage tool for the management, so outputing Data Science results to Google Sheets is important. We even have some small reports, which are in Google Sheets. The package allows for easy creating, finding, filling and reading of Google Sheets with an incredible simplicity of use.


# AnomalyDetection 

([Kirill's](/authors/#kirill) second pick. He gets to pick two since he is so indecisive)

[AnomalyDetection](https://github.com/twitter/AnomalyDetection) was developed by Twitter's data scientists and introduced to the open source community in the [first week of the year](https://blog.twitter.com/2015/introducing-practical-and-robust-anomaly-detection-in-a-time-series). A very handy, beautiful, well-developed tool to find anomalies in the data. This is very important for a data scientist to be able find anomalies in the data fast and reliable, before real damage occurs. The package allows to get a good first impression of the things going on in your KPIs (Key Performance Indicators) and react quick. Building alerts with it is a no-brainer if you want to monitor your data and assure data quality.


# emoGG 

<!-- jessica -->

